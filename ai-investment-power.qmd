---
title: "Power Analysis: AI Investment Decision Experiment"
subtitle: "Within-subjects design with 6 conditions"
author:
- "Jason Collins"
- "Iñigo De Juan Razquin"
- "Jianhua Li"
date: "`r Sys.Date()`"
format: 
  html:
    toc: true
    toc-location: left
    theme: flatly
    code-fold: true
    code-summary: "Show code"
    number-sections: true
execute:
  freeze: true
  cache: true
  cache.rebuild: false
  warning: false
  message: false
---

```{r chunk-options, include=FALSE}
knitr::opts_chunk$set(
  echo    = TRUE,    # hide code by default
  warning = FALSE,    # suppress warnings
  message = FALSE,    # suppress messages
  fig.width  = 8,     # consistent figure width
  fig.height = 5,     # consistent figure height
  fig.align  = "center",
  dpi        = 96     # reasonable resolution for html
)

```

```{r setup}
#| cache: FALSE

# Load required packages
library(tidyverse)
library(lme4)
library(geepack)
library(gridExtra)
library(knitr)
library(kableExtra)
library(simr)

# Set seed for reproducibility
set.seed(123)

# Global effect size parameters (percentage point increase)
PROP_CONTROL <- 0.75  # Control condition success rate (75%)
EFFECT_SIZE <- 0.03  # 3% increase: 75% → 78%
PROP_TREATMENT <- PROP_CONTROL + EFFECT_SIZE  # Treatment condition success rate (78%)

# Significance levels
ALPHA_STANDARD <- 0.05
ALPHA_BONFERRONI <- 0.01 

# Experimental design parameters
N_ROUNDS <- 6  # 1 control + 5 treatments
N_DECISIONS <- 6  # decisions per condition
SUBJECT_SD <- 0.177  # Subject-level SD for random effects, calculated from Germann and Merkle fund manager data
PRACTICE_EFFECT <- 0.01 # (0.01 = 1% improvement per round)
SAMPLE_SIZES <- c(200, 400, 600, 900, 1200)  # Sample sizes to test

# Quick mode for testing (set to FALSE for full analysis)
quick_mode <- TRUE
if (quick_mode) {
  N_SIMS <- 10  # Reduce for quick testing
} else {
  N_SIMS <- 500  # Full analysis
}

```


## Study Design Overview

This document provides power analysis for a within-subjects experiment with the following characteristics:

- **Design**: Within-subjects (repeated measures)
- **Participants**: Each participant completes 6 rounds, which each round being a separate condition
- **Conditions**: 1 control + 5 treatment conditions
- **Measurements**: 6 binary decisions per round (36 total per participant)
- **Outcome**: Score out of 6 for each round
- **Hypothesis**: Treatments will show 3% improvement over control (75% → 78% correct)
- **Target**: 90% power at α = 0.05
- **Multiple comparisons**: Bonferroni correction for 5 comparisons (α = 0.01)
- **Sample size range**: Up to 1200 participants

We tested three approaches:

1. **Paired t-test**: Compare participant scores out of 6 per condition
2. **GLMM**: Mixed-effects logistic regression on individual responses  
3. **GEE**: Generalised estimating equations with exchangeable correlation

All methods use Bonferroni correction (α = 0.01) for 5 treatment comparisons.

## Data simulation

The data is simulated based on the experimental design parameters. Each participant's decisions are generated with a random effect for each subject and a practice effect that increases the probability of correct decisions by 1% per round. The control condition has a base success rate of 75%, while each treatment condition has a 3% increase in success rate.

```{r data-simulation}

# General simulation function for all methods
simulate_data <- function(n_subjects,
                         p_control = PROP_CONTROL,
                         effect_size = EFFECT_SIZE,
                         n_decisions = N_DECISIONS,
                         n_rounds = N_ROUNDS,
                         subject_sd = SUBJECT_SD,
                         practice_effect = PRACTICE_EFFECT,
                         return_format = "binary") {
  
  # Total number of observations
  n_total <- n_subjects * n_rounds * n_decisions
  conditions <- c("control", paste0("treatment", 1:5))
  
  # Each subject gets each condition once in random order
  round_assignments <- replicate(n_subjects, sample(conditions), simplify = FALSE)
  round_assignments <- unlist(round_assignments)
  
  # Create vectors directly
  subject <- rep(rep(1:n_subjects, each = n_rounds), each = n_decisions)
  round <- rep(rep(1:n_rounds, n_subjects), each = n_decisions)
  decision <- rep(1:n_decisions, n_subjects * n_rounds)
  condition <- rep(round_assignments, each = n_decisions)
  
  # Generate subject effects once and index them
  subject_effects <- rep(rnorm(n_subjects, 0, subject_sd), each = n_rounds * n_decisions)
  
  # Vectorized probability calculations
  base_prob <- p_control + (condition != "control") * effect_size
  round_effect <- (round - 1) * practice_effect
  
  # Apply practice effect on probability scale first
  prob_with_practice <- base_prob + (round - 1) * practice_effect
  
  # Then convert to logit scale  
  logit_p <- qlogis(prob_with_practice) + subject_effects
  p <- plogis(logit_p)
  
  # Simulate outcomes
  correct <- rbinom(n_total, size = 1, prob = p)
  
  # Create data frame
  if (return_format == "binary") {
    df <- data.frame(
      subject = factor(subject),
      condition = factor(condition, levels = conditions),
      round = factor(round),
      decision = decision,
      correct = correct,
      is_treatment = as.integer(condition != "control"),
      stringsAsFactors = FALSE
    )
    return(df)
    
  } else {  # return_format == "scores"

    subject_condition <- paste(subject, condition, sep = "_")
    scores <- tapply(correct, subject_condition, sum)
    
    keys <- names(scores)
    subject_vals <- as.integer(sub("_.*", "", keys))
    condition_vals <- sub(".*_", "", keys)
    
    # CREATE the final dataframe directly (don't reference non-existent scores_df)
    return(data.frame(
      subject = factor(subject_vals),
      condition = factor(condition_vals, levels = conditions),
      score = as.vector(scores),
      stringsAsFactors = FALSE
    ))
  }
}

```

```{r data-generation-function}

# Pre-generate all datasets for all sample sizes
pregenerate_all_datasets <- function(sample_sizes = SAMPLE_SIZES, 
                                    n_sims = N_SIMS,
                                    effect_size = EFFECT_SIZE) {
  
  all_datasets <- list()
    
  for (n in sample_sizes) {
    
    datasets_binary <- list()
    datasets_scores <- list()
    
    for (i in 1:n_sims) {
      # Generate binary format once
      data_binary <- simulate_data(n, 
                                   effect_size = effect_size, 
                                   practice_effect = 0,
                                   return_format = "binary")
      
      # Calculate scores from the same binary data
      data_scores <- data_binary %>%
        group_by(subject, condition) %>%
        summarise(score = sum(correct), .groups = 'drop') %>%
        arrange(subject, condition)
      
      datasets_binary[[i]] <- data_binary
      datasets_scores[[i]] <- data_scores
    }
    
    all_datasets[[as.character(n)]] <- list(
      binary = datasets_binary,
      scores = datasets_scores
    )
  }  
  return(all_datasets)
}

```

```{r data-generation}
#| cache.vars: "all_datasets"

# Pre-generate all datasets once for all methods
all_datasets <- pregenerate_all_datasets()

```

## Paired t-test on Scores

This method compares the mean scores of each treatment condition against the control condition using paired t-tests.

```{r method1-functions}

# Analysis function that returns all p-values
analyse_paired_all <- function(data) {
  # Prepare data - control vs each treatment separately
  control_data <- data %>%
    filter(condition == "control") %>%
    select(subject, control_score = score)
  
  treatment_data <- data %>%
    filter(condition != "control") %>%
    left_join(control_data, by = "subject")
  
  # Perform 5 separate paired t-tests and return all results
  p_values_df <- treatment_data %>%
    group_by(condition) %>%
    summarise(
      p_value = {
        if (n() > 1 && sd(score - control_score) > 0) {
          t.test(score, control_score, paired = TRUE)$p.value
        } else {
          NA
        }
      },
      .groups = 'drop'
    )
  
  return(p_values_df)
}

# Power calculation function
calculate_power_paired <- function(datasets_scores) {
  
  n_sims <- length(datasets_scores)
  
  # Track successes for different tests
  treatment_significant_std <- rep(0, 5)
  treatment_significant_bonf <- rep(0, 5)
  
  for (i in 1:n_sims) {
    data <- datasets_scores[[i]]
    p_values_df <- analyse_paired_all(data)
    
    # Extract p-values
    p_values <- p_values_df$p_value[!is.na(p_values_df$p_value)]
    
    # Test each treatment separately
    for (j in 1:5) {
      treatment_name <- paste0("treatment", j)
      p_val <- p_values_df$p_value[p_values_df$condition == treatment_name]
      if (length(p_val) > 0 && !is.na(p_val)) {
        if (p_val < ALPHA_STANDARD) {
          treatment_significant_std[j] <- treatment_significant_std[j] + 1
        }
        if (p_val < ALPHA_BONFERRONI) {
          treatment_significant_bonf[j] <- treatment_significant_bonf[j] + 1
        }
      }
    }
  }

  # Calculate power
  power_treatment_std <- treatment_significant_std / n_sims
  power_treatment_bonf <- treatment_significant_bonf / n_sims

  return(list(
    power_average_std = mean(power_treatment_std),
    power_average_bonf = mean(power_treatment_bonf),
    power_detail_std = power_treatment_std,
    power_detail_bonf = power_treatment_bonf
  ))
}

```

```{r method1-power}

# Calculate power for each sample size
powers_average_std <- numeric(length(SAMPLE_SIZES))
powers_average_bonf <- numeric(length(SAMPLE_SIZES))

for (i in seq_along(SAMPLE_SIZES)) {
  n <- SAMPLE_SIZES[i]
  
  # Use pre-generated datasets
  datasets_scores <- all_datasets[[as.character(n)]]$scores
  
  power_results <- calculate_power_paired(datasets_scores)
  powers_average_std[i] <- power_results$power_average_std
  powers_average_bonf[i] <- power_results$power_average_bonf
}

```

```{r method1-table}
#| cache: FALSE

# Create data frame with results for all tested sample sizes for Method 1
paired_results <- data.frame(
  Sample_Size = SAMPLE_SIZES,
  Power_Standard = round(powers_average_std, 2),
  Power_Bonferroni = round(powers_average_bonf, 2)
)

# Display results in a table
kable(paired_results, 
      caption = "Power by Sample Size (Average across 5 treatments)",
      col.names = c("Sample Size", 
                    "α=0.05",
                    "α=0.01"),
      align = c("l", "c", "c")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```

```{r method1-plot}
#| cache: FALSE

# Create comparison plot with the power curves
power_data <- data.frame(
  n = rep(SAMPLE_SIZES, 2),
  power = c(powers_average_std, powers_average_bonf),
  correction = rep(c("No correction (α=0.05)", "Bonferroni (α=0.01)"), 
                   each = length(SAMPLE_SIZES))
)

# Plot the power curves
ggplot(power_data, aes(x = n, y = power, colour = correction)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  geom_hline(yintercept = 0.90, linetype = "dashed", colour = "red") +
  scale_y_continuous(breaks = seq(0, 1, 0.1), limits = c(0, 1)) +
  scale_x_continuous(breaks = seq(0, 1200, 100)) +
  scale_colour_manual(values = c(
    "No correction (α=0.05)" = "darkblue",
    "Bonferroni (α=0.01)" = "darkgreen"
  )) +
  labs(x = "Number of Participants",
       y = "Statistical Power",
       title = "Method 1: Paired t-test (Average Power across Treatments)",
       colour = "Significance Level") +
  theme_minimal() +
  theme(legend.position = "bottom")

```

## Mixed Effects Models (GLMM)

This approach models the binary outcomes directly using logistic regression with random effects.

```{r method2-functions}

# GLMM analysis
analyse_glmm <- function(data, alpha = 0.05) {
  tryCatch({
    # Ensure consistent factor levels
    data$condition <- factor(data$condition, levels = c("control", paste0("treatment", 1:5)))
    
    # Primary model: condition effects only with random intercepts
    suppressWarnings({
      model <- glmer(correct ~ condition + round + (1|subject), 
                     data = data, 
                     family = binomial,
                     control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 20000)))
    })
    
    # If singular, the model is already as simple as reasonable for this design
    if (isSingular(model)) {
      warning("Model is singular - consider checking data or reducing complexity")
    }
    
    # Extract coefficients
    coef_summary <- summary(model)$coefficients
    
    # Find treatment coefficients (control is reference level)
    treatment_rows <- grep("conditiontreatment", rownames(coef_summary))
    
    if (length(treatment_rows) == 0) return(NA)
    
    # Extract p-values for each treatment vs control comparison
    p_values <- coef_summary[treatment_rows, "Pr(>|z|)"]
    
    # Return all p-values as a named vector
    p_values <- coef_summary[treatment_rows, "Pr(>|z|)"]
    names(p_values) <- paste0("treatment", 1:5)
    return(p_values)
    
  }, error = function(e) {
    return(rep(NA,5))
  })
}

# Calculate power for GLMM
calculate_power_glmm <- function(datasets_binary) {
  
  n_sims <- length(datasets_binary)
  
  # Initialize counters
  treatment_significant_std <- rep(0, 5)
  treatment_significant_bonf <- rep(0, 5)
  valid_simulations <- 0
  
  for (i in 1:n_sims) {
    data <- datasets_binary[[i]]
    p_vals <- analyse_glmm(data)
    
    if (!is.na(p_vals[1])) {
      valid_simulations <- valid_simulations + 1
      
      for (j in 1:5) {
        if (!is.na(p_vals[j])) {
          if (p_vals[j] < ALPHA_STANDARD) {
            treatment_significant_std[j] <- treatment_significant_std[j] + 1
          }
          if (p_vals[j] < ALPHA_BONFERRONI) {
            treatment_significant_bonf[j] <- treatment_significant_bonf[j] + 1
          }
        }
      }
    }
  }
  
  if (valid_simulations == 0) {
    return(list(power_average_std = NA, power_average_bonf = NA, 
                power_detail_std = NA, power_detail_bonf = NA))
  }
  
  return(list(
    power_average_std = mean(treatment_significant_std / valid_simulations),
    power_average_bonf = mean(treatment_significant_bonf / valid_simulations),
    power_detail_std = treatment_significant_std / valid_simulations,
    power_detail_bonf = treatment_significant_bonf / valid_simulations
  ))
}

```

```{r method2-power}

# Initialize vectors to store results
glmm_standard <- numeric(length(SAMPLE_SIZES))
glmm_bonferroni <- numeric(length(SAMPLE_SIZES))

for (i in seq_along(SAMPLE_SIZES)) {
  n <- SAMPLE_SIZES[i]
  
  # Use pre-generated datasets
  datasets_binary <- all_datasets[[as.character(n)]]$binary
  
  power_results <- calculate_power_glmm(datasets_binary)
  glmm_standard[i] <- power_results$power_average_std
  glmm_bonferroni[i] <- power_results$power_average_bonf
}

```

```{r method2-table}
#| cache: FALSE

# Create results data frame
glmm_results <- data.frame(
  Sample_Size = SAMPLE_SIZES,
  Power_Standard = round(glmm_standard, 2),
  Power_Bonferroni = round(glmm_bonferroni, 2)
)

# Create a summary table for GLMM results
kable(glmm_results, 
      caption = "Power estimates using GLMM",
      col.names = c("Sample Size", 
                    "α=0.05",
                    "α=0.01"),
      align = c("l", "c", "c")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")
  )

```

### Validation with `simr`

To validate the GLMM power estimates, we can use the `simr` package to calculate the power curve for detecting a single treatment effect. This provides a per-comparison power estimate using the Bonferroni-corrected significance level..

First, we need to create a base model using the simulated data.

```{r method2-simr-model}

# Create the base model as a template
initial_n <- 100
initial_data_glmm <- simulate_data(initial_n) %>%
  mutate(round = factor(round))

model_formula <- correct ~ condition + round + (1|subject)

base_model <- glmer(
  model_formula,
  data = initial_data_glmm,
  family = binomial,
  control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 20000))
)

# Extend the model to the maximum desired sample size
glmm_extended <- extend(base_model, along = "subject", n = 1200)

# Set the model parameters
current_fixef <- fixef(glmm_extended)
current_fixef["(Intercept)"] <- qlogis(PROP_CONTROL)
target_log_or <- qlogis(PROP_TREATMENT) - qlogis(PROP_CONTROL)
treatment_coef_names <- names(current_fixef)[startsWith(names(current_fixef), "conditiontreatment")]
current_fixef[treatment_coef_names] <- target_log_or
fixef(glmm_extended) <- current_fixef

# Set the random effects
VarCorr(glmm_extended)$subject[1] <- SUBJECT_SD^2

# Test for a single treatment comparison
comparison_test <- fixed("conditiontreatment1", "z")

```

We then extend this model to simulate the power curve across a range of sample sizes.

```{r method2-simr-powercurve}

# Calculate power curve for standard alpha (0.05)
power_curve_standard <- powerCurve(
  glmm_extended,
  test = comparison_test,
  along = "subject",
  breaks = SAMPLE_SIZES,
  nsim = N_SIMS,
  alpha = ALPHA_STANDARD,
  progress = FALSE
)

power_curve_bonferroni <- powerCurve(
  glmm_extended,
  test = comparison_test,
  along = "subject",
  breaks = SAMPLE_SIZES,
  nsim = N_SIMS,
  alpha = ALPHA_BONFERRONI,
  progress = FALSE
)

```

We then plot the power curve to visualize how the power changes with sample size.

```{r method2-simr-plot}
#| cache: FALSE

# Plot both power curves
cat("Power curve with standard alpha (0.05):\n")
plot(power_curve_standard)
cat("Power curve with Bonferroni alpha (0.01):\n")
plot(power_curve_bonferroni)

```

The following code extracts the power values from the `simr` power curve and formats them into a summary table.

```{r method2-simr-table}
#| cache: FALSE

# Extract and combine results
power_data_standard <- summary(power_curve_standard)
power_data_bonf <- summary(power_curve_bonferroni)

# Create combined table
power_results_combined <- data.frame(
  N = power_data_standard$nlevels,
  Power_Standard = sprintf("%.1f%%", power_data_standard$mean * 100),
  CI_Standard = sprintf("[%.1f%% - %.1f%%]", 
                        power_data_standard$lower * 100, 
                        power_data_standard$upper * 100),
  Power_Bonferroni = sprintf("%.1f%%", power_data_bonf$mean * 100),
  CI_Bonferroni = sprintf("[%.1f%% - %.1f%%]", 
                          power_data_bonf$lower * 100, 
                          power_data_bonf$upper * 100)
)

kable(power_results_combined,
      caption = "Power using simr at both significance levels",
      col.names = c("Sample Size", "Power (α=0.05)", "95% CI", 
                    "Power (α=0.01)", "95% CI"),
      align = c("l", "c", "c", "c", "c")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```


## Generalised Estimating Equations (GEE)

GEE is a robust alternative that handles within-subject correlation using a marginal modelling approach, avoiding the convergence issues that can occur with mixed-effects models while still accounting for the repeated measures structure. The following code implements the GEE analysis to  estimate power for detecting any significant treatment effects at the population level.

```{r method3-functions}

# GEE analysis
analyse_gee <- function(data, alpha = 0.05) {
  tryCatch({    
    suppressWarnings({
      model <- geeglm(correct ~ condition + round, 
                      id = subject,
                      data = data, 
                      family = binomial,
                      corstr = "exchangeable"
                      )
    })
    
    # Extract coefficients
    coef_summary <- summary(model)$coefficients
    
    # Find treatment coefficients (control is reference level)
    treatment_rows <- grep("conditiontreatment", rownames(coef_summary))
    
    if (length(treatment_rows) == 0) return(rep(NA, 5))
    
    # Extract p-values for each treatment vs control comparison
    p_values <- coef_summary[treatment_rows, "Pr(>|W|)"]
    names(p_values) <- paste0("treatment", 1:5)
    
    # Return all p-values (for consistency with other methods)
    return(p_values)
    
  }, error = function(e) {
    return(rep(NA, 5))
  })
}

# Power calculation for GEE
calculate_power_gee <- function(datasets_binary) {
  
  n_sims <- length(datasets_binary)
  
  # Initialize counters
  treatment_significant_std <- rep(0, 5)
  treatment_significant_bonf <- rep(0, 5)
  valid_simulations <- 0
  
  for (i in 1:n_sims) {
    data <- datasets_binary[[i]]
    p_vals <- analyse_gee(data)
    
    if (!all(is.na(p_vals))) {
      valid_simulations <- valid_simulations + 1
      
      for (j in 1:5) {
        if (!is.na(p_vals[j])) {
          if (p_vals[j] < ALPHA_STANDARD) {
            treatment_significant_std[j] <- treatment_significant_std[j] + 1
          }
          if (p_vals[j] < ALPHA_BONFERRONI) {
            treatment_significant_bonf[j] <- treatment_significant_bonf[j] + 1
          }
        }
      }
    }
  }
  
  if (valid_simulations == 0) {
    return(list(power_average_std = NA, power_average_bonf = NA))
  }
  
  return(list(
    power_average_std = mean(treatment_significant_std / valid_simulations),
    power_average_bonf = mean(treatment_significant_bonf / valid_simulations)
  ))
}

```

```{r method3-power}

# GEE power calculation loop
gee_power_std <- numeric(length(SAMPLE_SIZES))
gee_power_bonf <- numeric(length(SAMPLE_SIZES))

for (i in seq_along(SAMPLE_SIZES)) {
  n <- SAMPLE_SIZES[i]
  
  # Use pre-generated datasets
  datasets_binary <- all_datasets[[as.character(n)]]$binary
  
  power_results <- calculate_power_gee(datasets_binary)
  gee_power_std[i] <- power_results$power_average_std
  gee_power_bonf[i] <- power_results$power_average_bonf
}

```

```{r method3-table}
#| cache: FALSE

# Create results data frame for GEE
gee_results <- data.frame(
  Sample_Size = SAMPLE_SIZES,
  Power_Standard = round(gee_power_std, 2),
  Power_Bonferroni = round(gee_power_bonf, 2)
)

# Display results
kable(gee_results, 
      caption = "Power estimates using GEE",
      col.names = c("Sample Size", 
                    "α=0.05",
                    "α=0.01"),
      align = c("l", "c", "c")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```


## Minimum Detectable Effect

With the maximum planned sample size of 1200 participants, and using the methods above, this analysis estimates the minimum effect sizes required to achieve 90% power:

```{r min-detectable-effect-calc}

# Define the function
find_minimum_detectable_effect <- function(method_name, 
                                         n_subjects = 1200,
                                         power_threshold = 0.90) {
  
  effect_sizes <- seq(0.01, 0.05, by = 0.001)
  
  # Track MDE for both alpha levels
  mde_standard <- NA
  mde_bonferroni <- NA
  
  for (effect in effect_sizes) {
    # Generate datasets with the current effect size
    datasets_binary <- list()
    datasets_scores <- list()
    
    for (i in 1:N_SIMS) {
      data_binary <- simulate_data(n_subjects, 
                                 effect_size = effect, 
                                 practice_effect = PRACTICE_EFFECT,
                                 return_format = "binary")
      
      data_scores <- simulate_data(n_subjects, 
                                 effect_size = effect, 
                                 practice_effect = PRACTICE_EFFECT,
                                 return_format = "scores")
      
      datasets_binary[[i]] <- data_binary
      datasets_scores[[i]] <- data_scores
    }
    
    # Use the appropriate function with the generated datasets
    if (method_name == "paired") {
      power <- calculate_power_paired(datasets_scores)
    } else if (method_name == "glmm") {
      power <- calculate_power_glmm(datasets_binary)
    } else if (method_name == "gee") {
      power <- calculate_power_gee(datasets_binary)
    }
    
    # Check if we've found MDE for standard alpha
    if (is.na(mde_standard) && !is.na(power$power_average_std) && 
        power$power_average_std >= power_threshold) {
      mde_standard <- effect
    }
    
    # Check if we've found MDE for Bonferroni alpha
    if (is.na(mde_bonferroni) && !is.na(power$power_average_bonf) && 
        power$power_average_bonf >= power_threshold) {
      mde_bonferroni <- effect
    }
    
    # Stop if we've found both
    if (!is.na(mde_standard) && !is.na(mde_bonferroni)) {
      break
    }
  }
  
  return(list(
    mde_standard = mde_standard,
    mde_bonferroni = mde_bonferroni
  ))
}

# Calculate MDE for all methods
mde_paired <- find_minimum_detectable_effect("paired")
mde_glmm <- find_minimum_detectable_effect("glmm")
mde_gee <- find_minimum_detectable_effect("gee")

mde_results <- data.frame(
  Method = c("Paired t-test", "GLMM", "GEE"),
  MDE_Standard = c(mde_paired$mde_standard, 
                   mde_glmm$mde_standard, 
                   mde_gee$mde_standard),
  MDE_Bonferroni = c(mde_paired$mde_bonferroni, 
                     mde_glmm$mde_bonferroni, 
                     mde_gee$mde_bonferroni)
)

```

```{r min-detectable-effect-table}

# Format percentages
mde_results <- mde_results %>%
  mutate(
    MDE_Standard = ifelse(is.na(MDE_Standard), 
                         "> 5.0%",
                         sprintf("%.1f%%", MDE_Standard * 100)),
    MDE_Bonferroni = ifelse(is.na(MDE_Bonferroni), 
                           "> 5.0%",
                           sprintf("%.1f%%", MDE_Bonferroni * 100))
  )

# Create table
kable(mde_results,
      caption = "Minimum Detectable Effects with N=1200 participants (90% power)",
      col.names = c("Method", "α = 0.05", "α = 0.01 (Bonferroni)"),
      align = c("l", "c", "c")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

```